---
title: "GEO879 - Mobility Issues Project"
author: "Cyril Geistlich, Annika Kunz, Fabienne Koenig"
date: "2023-10-11"
output: html_document
---

# Load Libraries

```{r setup, include=FALSE, warning = F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Install/load packages
## Default repository
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos = r)
})

check_pkg <- function(x)
  {
    if (!require(x, character.only = TRUE))
    {
      install.packages(x, dep = TRUE)
        if(!require(x, character.only = TRUE)) stop("Package not found")
    }
}

check_pkg("sf")
check_pkg("httr")
check_pkg("ggplot2")
check_pkg("plotly")
check_pkg("tidyverse")
check_pkg("leaflet")
check_pkg("dplyr")
check_pkg("XML")
check_pkg("mapview")
check_pkg("lubridate")
check_pkg("dplyr")
check_pkg("readr") 
check_pkg("tmap")
check_pkg("spatstat") 
check_pkg("dbscan")
check_pkg("ellipse")
check_pkg("phonTools")
check_pkg("devtools")
check_pkg("stringr")
check_pkg("zoo")

```

```{r Projections}
# Projection strings for the Swiss LV03 & LV95 CRS and WGS84 CRS, respectively
crs_lv03  <- 21781
crs_lv95  <- 2056
crs_wgs84 <- 4326
```


## Read Data

```{r}
# I use .gpkg, since it is delivererd in Lv95 and no further tranformation is needed. 
accidents <- st_read( "data/RoadTrafficAccidentLocations.gpkg")
zh_boundary <- st_read( "data/Zurich_city_boundary.gpkg")
zh_districts <- st_read( "data/Zurich_city_districts.gpkg")
```
All layers are in CH1903+ / LV95. Therefore no further transformations are needed. 

## Data Preview

```{r Tables}
# a)
accidents |> ggplot() +
  geom_bar(aes(AccidentSeverityCategory_en)) +
  xlab("Accident Severity Category") +
  ggtitle("Number of Accidents by Severity") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# b)
accidents |> ggplot() +
  geom_bar(aes(AccidentType_en)) +
  xlab("Accident Category") +
  ggtitle("Number of Accidents by Category") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# c)

p <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == F)

b <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

m <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

pb <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

pm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

bm <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)

pbm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)


counts_table <- data.frame(
  Type = c("Pedestrian","Bicycle","Motorcycle","Pedestrian & Bicycle", "Pedestrian & Motorcycle", "Bicycle & Motorcycle", "Pedestrian & Bicycle & Motorcycle"),
  Count = c(nrow(p),nrow(b),nrow(m),nrow(pb), nrow(pm), nrow(bm), nrow(pbm))
)

print(counts_table)


```

## Temporal Evolution

```{r temporal evolution plot}

temporal_plot <- accidents |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

temporal_plot_b <- b |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

combined_plot <- ggplot() +
  geom_line(data = temporal_plot$data, aes(x = AccidentYear, y = Count, linetype = "All Accidents"), size = 1, color = "black") +
  geom_line(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, linetype = "Bicycle Accidents"), size = 1, color = "red") +
  geom_text(data = temporal_plot$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, hjust = -0.5, size = 3, color = "black") +
  geom_text(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, size = 3, color = "red") +
  xlab("Year") +
  ylab("Number of Accidents") +
  ggtitle("Accidents Over the Years") +
  scale_x_continuous(breaks = unique(accidents$AccidentYear)) +
  scale_linetype_manual(values = c("All Accidents" = "solid", "Bicycle Accidents" = "solid")) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "red"))) )  +
  scale_color_manual(values = c("All Accidents" = "black", "Bicycle Accidents" = "red")) +
  labs(linetype = "Transport Mode Type") + 
  geom_point(data = temporal_plot$data,  aes(x = AccidentYear, y = Count)) +
  geom_point(data = temporal_plot_b$data,  aes(x = AccidentYear, y = Count), color = "red")

print(combined_plot)

```

## Accident Severity

```{r Tmap Accident Severity}

tmap_mode("view")

basemap_imagery <-tm_basemap("Esri.WorldTopoMap") +
  tm_basemap("Esri.WorldImagery") +
  tm_basemap("Esri.WorldStreetMap") +
  tm_basemap("Esri.WorldGrayCanvas")

accident_map <- tm_shape(b) +
  tm_dots(col = "AccidentSeverityCategory_en", palette = "Set3", size = 0.02) + 
  tm_layout(
    title = "Accident Severity Map of Zurich", 
    title.position = c('left', 'top'), 
    legend.frame = T
  ) +
  tm_shape(zh_boundary) +
  tm_borders() +
  tm_shape(zh_districts) +
  tm_borders() +
  basemap_imagery 

accident_map

```

# Clustering of Accidents

Most accidents occur on or close to intersections. We have accumulations of accidents along larger streets. This might be the case because those streets might be cycled on more frequently and at a higher average velocity. 

```{r dbscan}

sf_data <- b
years <- c(2018, 2019, 2020, 2021)
cluster_maps <- list()

for (year in years) {
  data_year <- sf_data[sf_data$AccidentYear == year, ]
  coordinates_df <- st_coordinates(data_year)

  dbscan_result <- dbscan(coordinates_df, eps = 125)

  # move cluster 0 to own layer
  data_year_cluster0 <- data_year[dbscan_result$cluster == 0, ]
  data_year_cluster0$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster == 0])
  
  data_year <- data_year[dbscan_result$cluster != 0, ]
  data_year$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster != 0])

  cluster_map <- tm_shape(data_year) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tm_shape(data_year_cluster0) +
    tm_dots(col = "Cluster", size = 0.01) +
    tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", year),
      title.position = c('left', 'top'),
      legend.frame = TRUE
    )

  cluster_maps[[as.character(year)]] <- cluster_map

  assign(paste0("b_", year), data_year)
}

for (year in years) {
  print(cluster_maps[[as.character(year)]])
}

```


Cluster 0 was filtered out, since the data points in the cluster were spread out through the whole city. After removing the other smaller clusters become better visible in the maps. The clusters are along larger roads or at large intersection with a lot of traffic.

```{r SDE, include = F}

# Create a list to store ellipses
ellipses <- data.frame()
clusters <- unique(b_2018$Cluster)
ellipses_list <- list()

# 2018
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2018[b_2018$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
    
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2018 <- do.call(rbind, ellipses_list)
}

# 2021
clusters <- unique(b_2021$Cluster)
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2021[b_2021$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
  
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2021 <- do.call(rbind, ellipses_list)
}

```


```{r ellipse map}
ellipse_map_18 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2018) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2018) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2018),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )

ellipse_map_21 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2021) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2021) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2021),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )
ellipse_map_18
ellipse_map_21
```


## Track Data


```{r load GPX data}
gpx_parsed_A <- htmlTreeParse(file = "data/Geo879_route_Annika.gpx", useInternalNodes = TRUE)
gpx_parsed_F <- htmlTreeParse(file = "data/Geo879_route_Fabienne.gpx", useInternalNodes = TRUE)
gpx_parsed_C <- htmlTreeParse(file = "data/Geo879_route_Cyril.gpx", useInternalNodes = TRUE)

```

Extract and store them in a more readable structure:
```{r extract info to dataframe}
coords <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt/ele", fun = xmlValue)
ts_chr <- xpathSApply(gpx_parsed_A, path = "//trkpt/time", xmlValue)

track <- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = "UTC"),
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

# Add index to df
track$index <- 1:nrow(track)
```

```{r convert to sf object and build trajectory}
track_sf <- st_as_sf(x = track, coords = c("lon","lat"), crs = 4326) %>% arrange(index)
mapview(track_sf)

# Creating trajectory by combining the coordinate points into multipoint feature
traj <- st_union(track_sf$geometry)
traj_sf <- st_sf(geometry = traj) %>% st_cast("LINESTRING")
# mapview(traj_sf) # NOT WORKING PROPERLY > ORDER IS NOT CORRECT
```

## Read Empatica 4 Data

# EDA - Electrodermal Activity (Unit: microsiemens)

```{r}
EDA_A <- read_csv("data/E4_data_Annika/EDA.csv")
EDA_F <- read_csv("data/E4_data_Fabienne/EDA.csv")
temp_A <- read_csv("data/E4_data_Annika/tags.csv")


```

# Following the tutorial to parse the E4 data https://rpubs.com/OliverSW/eda-analysis-tutorial
# Adding timestamp info

```{r write functions to parse E4 data and add timestamp}

read.empatica.eda <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(Timestamp=NA, EDA=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$EDA) 
  data$Timestamp <- timestamps
  data
}

read.empatica.acc <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(X=raw$V1[3:length(raw$V1)]/64.0,Y=raw$V2[3:length(raw$V2)]/64.0,Z=raw$V3[3:length(raw$V3)]/64.0)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$X) 
  data$Timestamp <- timestamps
  data
}

read.empatica.bvp <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(BVP=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$BVP) 
  data$Timestamp <- timestamps
  data
}

read.empatica.hr <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(seq_along(raw$V1[2:length(raw$V1)]),units = "secs")
  timestamps <- start+dt
  HR <- as.numeric(raw$V1[2:length(raw$V1)])
  data <- data.frame(Timestamp=timestamps, HR=HR)
  data
}

read.empatica.ibi <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(raw$V1[2:length(raw$V1)],units = "secs")
  timestamps <- start+dt
  ibi <- as.double(as.character(raw$V2[2:length(raw$V2)]) )
  data <- data.frame(Timestamp=timestamps, IBI=ibi)
  data
}

read.empatica.temp <- function(file) {
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  temperatureF <- (9.0/5.0)*(raw$V1[3:length(raw$V1)] + 32.0)
  data <- data.frame(TEMP=temperatureF)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$TEMP) 
  data$Timestamp <- timestamps
  data
}

read.empatica <- function(path) {
  FIELDS <- c("Z","Y","X","Battery","Temperature","EDA")
  FILES <- c("ACC.csv","BVP.csv","EDA.csv","TEMP.csv","IBI.csv")
  #First lets read the file header info
  acc <- read.empatica.acc(file.path(path,"ACC.csv"))
  bvp <- read.empatica.bvp(file.path(path,"BVP.csv"))
  ibi <- read.empatica.ibi(file.path(path,"IBI.csv"))
  temp <- read.empatica.temp(file.path(path,"TEMP.csv"))
  eda <- read.empatica.eda(file.path(path,"EDA.csv"))
  
  data <- list(ACC=acc,BVP=bvp,EDA=eda,TEMP=temp,IBI=ibi)
  attr(data,"class") <- "eda"
  return(data)
}
```

# Read EDA data

```{r read E4 data}
# Data Annika 
e4Data_A <- read.empatica("data/E4_data_Annika/")
EDA_A <- e4Data_A$EDA
EDA_A

# Data Fabi
e4Data_F <- read.empatica("data/E4_data_Fabienne/")
EDA_F <- e4Data_F$EDA
EDA_F
```

```{r visualizing EDA values}
plt_A <- qplot(data=EDA_A,x = Timestamp, y=EDA,geom="line",col="Annika") 
ggplotly(plt_A,width = 900,height=450)
```

```{r}
plt_F <- qplot(data=EDA_F,x = Timestamp, y=EDA,geom="line",col="Fabienne") 
ggplotly(plt_F,width = 900,height=450)
```

```{r Filter & Smoothing}

EDA_A_filtered <- EDA_A |>
  mutate(Q1 = quantile(EDA, 0.25),
         Q3 = quantile(EDA, 0.75),
         IQR = Q3 - Q1) |>
  filter(EDA < (Q3 + 1.5 * IQR)) # Adjust the multiplier for more stringent or lenient filtering


window_size <- 5

EDA_A_smoothed <- EDA_A_filtered |>
  mutate(GSR_smoothed = rollmean(EDA, k = window_size, fill = NA))

plt_A <- qplot(data=EDA_A_smoothed,x = Timestamp, y=GSR_smoothed,geom="line",col="Annika") 
ggplotly(plt_A,width = 900,height=450)
```



```{r}
# Assuming 'EDA_A_filtered' is a data frame with columns 'Timestamp' and 'EDA'


window_size <- 32  # Adjust the window size as needed

# https://tesi.univpm.it/retrieve/7fd49ab6-6325-406a-867d-68c75c1d2375/Thesis%20-%20Anna%20Brocanelli.pdf -> Window size according to page 30.

# Calculate the rolling mean using a window
EDA_A_smoothed$tonic_values <- rollmean(EDA_A_smoothed$EDA, k = window_size, align = "center", fill = NA)

# Plotting the original EDA measurements and the smoothed trend
plot(EDA_A_filtered$Timestamp, EDA_A_smoothed$EDA, type = 'l', col = 'blue', xlab = 'Timestamp', ylab = 'EDA Measurement') +
lines(EDA_A_filtered$Timestamp, EDA_A_smoothed$tonic_values, col = 'red', lwd = 2)

```

```{r}
# Compute Phasic Values
EDA_A_smoothed$phasic_values <- EDA_A_smoothed$GSR_smoothed - EDA_A_smoothed$tonic_values

plt_A_phasic <- qplot(data=EDA_A_smoothed,x = Timestamp, y=phasic_values,geom="line",col="Annika") 
ggplotly(plt_A_phasic,width = 900,height=450)
```

```{r}
# Extract peaks and timestamp
peaksize <- 0.25


# Find peaks exceeding the threshold
peak_positions <- which(EDA_A_filtered$EDA > peaksize)
peak_timestamps <- EDA_A_filtered$Timestamp[peak_positions]

# Display the positions and timestamps of the peaks
peak_data <- data.frame(Position = peak_positions, Timestamp = peak_timestamps)
print(peak_data)

```

```{r}
# Next: Find positions in Track Data and identify locations. 
```

