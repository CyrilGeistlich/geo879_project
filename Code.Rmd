---
title: "GEO879 - Mobility Issues Project"
author: "Cyril Geistlich, Annika Kunz, Fabienne Koenig"
date: "2023-10-11"
output: html_document
---

# Load Libraries

```{r setup, include=FALSE, warning = F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Install/load packages
## Default repository
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos = r)
})

check_pkg <- function(x)
  {
    if (!require(x, character.only = TRUE))
    {
      install.packages(x, dep = TRUE)
        if(!require(x, character.only = TRUE)) stop("Package not found")
    }
}

check_pkg("sf")
check_pkg("httr")
check_pkg("ggplot2")
check_pkg("plotly")
check_pkg("tidyverse")
check_pkg("leaflet")
check_pkg("dplyr")
check_pkg("XML")
check_pkg("mapview")
check_pkg("lubridate")
check_pkg("dplyr")
check_pkg("readr") 
check_pkg("tmap")
check_pkg("spatstat") 
check_pkg("dbscan")
check_pkg("ellipse")
check_pkg("phonTools")
check_pkg("devtools")
check_pkg("stringr")
check_pkg("zoo")
check_pkg("RColorBrewer")
check_pkg("spatstat")
check_pkg("car")

```

```{r Projections}
# Projection strings for the Swiss LV03 & LV95 CRS and WGS84 CRS, respectively
crs_lv03  <- 21781
crs_lv95  <- 2056
crs_wgs84 <- 4326
```


## Read Data

```{r}
# I use .gpkg, since it is delivererd in Lv95 and no further tranformation is needed. 
accidents <- st_read( "data/RoadTrafficAccidentLocations.gpkg")
zh_boundary <- st_read( "data/Zurich_city_boundary.gpkg")
zh_districts <- st_read( "data/Zurich_city_districts.gpkg")
```
All layers are in CH1903+ / LV95. Therefore no further transformations are needed. 

## Data Preview

```{r Tables}
# a)
accidents |> ggplot() +
  geom_bar(aes(AccidentSeverityCategory_en)) +
  xlab("Accident Severity Category") +
  ggtitle("Number of Accidents by Severity") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# b)
accidents |> ggplot() +
  geom_bar(aes(AccidentType_en)) +
  xlab("Accident Category") +
  ggtitle("Number of Accidents by Category") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# c)

p <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == F)

b <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

m <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

pb <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

pm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

bm <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)

pbm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)


counts_table <- data.frame(
  Type = c("Pedestrian","Bicycle","Motorcycle","Pedestrian & Bicycle", "Pedestrian & Motorcycle", "Bicycle & Motorcycle", "Pedestrian & Bicycle & Motorcycle"),
  Count = c(nrow(p),nrow(b),nrow(m),nrow(pb), nrow(pm), nrow(bm), nrow(pbm))
)

print(counts_table)


```

## Temporal Evolution

```{r temporal evolution plot}

temporal_plot <- accidents |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

temporal_plot_b <- b |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

combined_plot <- ggplot() +
  geom_line(data = temporal_plot$data, aes(x = AccidentYear, y = Count, linetype = "All Accidents"), size = 1, color = "black") +
  geom_line(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, linetype = "Bicycle Accidents"), size = 1, color = "red") +
  geom_text(data = temporal_plot$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, hjust = -0.5, size = 3, color = "black") +
  geom_text(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, size = 3, color = "red") +
  xlab("Year") +
  ylab("Number of Accidents") +
  ggtitle("Accidents Over the Years") +
  scale_x_continuous(breaks = unique(accidents$AccidentYear)) +
  scale_linetype_manual(values = c("All Accidents" = "solid", "Bicycle Accidents" = "solid")) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "red"))) )  +
  scale_color_manual(values = c("All Accidents" = "black", "Bicycle Accidents" = "red")) +
  labs(linetype = "Transport Mode Type") + 
  geom_point(data = temporal_plot$data,  aes(x = AccidentYear, y = Count)) +
  geom_point(data = temporal_plot_b$data,  aes(x = AccidentYear, y = Count), color = "red")

print(combined_plot)

```

## Accident Severity

```{r Tmap Accident Severity}

tmap_mode("view")

basemap_imagery <-tm_basemap("Esri.WorldTopoMap") +
  tm_basemap("Esri.WorldImagery") +
  tm_basemap("Esri.WorldStreetMap") +
  tm_basemap("Esri.WorldGrayCanvas")

accident_map <- tm_shape(b) +
  tm_dots(col = "AccidentSeverityCategory_en", palette = "Set3", size = 0.02) + 
  tm_layout(
    title = "Accident Severity Map of Zurich", 
    title.position = c('left', 'top'), 
    legend.frame = T
  ) +
  tm_shape(zh_boundary) +
  tm_borders() +
  tm_shape(zh_districts) +
  tm_borders() +
  basemap_imagery 

accident_map

```

# Clustering of Accidents

Most accidents occur on or close to intersections. We have accumulations of accidents along larger streets. This might be the case because those streets might be cycled on more frequently and at a higher average velocity. 

```{r dbscan}

sf_data <- b
years <- c(2018, 2019, 2020, 2021)
cluster_maps <- list()

for (year in years) {
  data_year <- sf_data[sf_data$AccidentYear == year, ]
  coordinates_df <- st_coordinates(data_year)

  dbscan_result <- dbscan(coordinates_df, eps = 125)

  # move cluster 0 to own layer
  data_year_cluster0 <- data_year[dbscan_result$cluster == 0, ]
  data_year_cluster0$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster == 0])
  
  data_year <- data_year[dbscan_result$cluster != 0, ]
  data_year$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster != 0])

  cluster_map <- tm_shape(data_year) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tm_shape(data_year_cluster0) +
    tm_dots(col = "Cluster", size = 0.01) +
    tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", year),
      title.position = c('left', 'top'),
      legend.frame = TRUE
    )

  cluster_maps[[as.character(year)]] <- cluster_map

  assign(paste0("b_", year), data_year)
}

for (year in years) {
  print(cluster_maps[[as.character(year)]])
}

```


Cluster 0 was filtered out, since the data points in the cluster were spread out through the whole city. After removing the other smaller clusters become better visible in the maps. The clusters are along larger roads or at large intersection with a lot of traffic.

```{r SDE, include = F}

# Create a list to store ellipses
ellipses <- data.frame()
clusters <- unique(b_2018$Cluster)
ellipses_list <- list()

# 2018
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2018[b_2018$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
    
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2018 <- do.call(rbind, ellipses_list)
}

# 2021
clusters <- unique(b_2021$Cluster)
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2021[b_2021$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
  
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2021 <- do.call(rbind, ellipses_list)
}

```


```{r ellipse map}
ellipse_map_18 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2018) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2018) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2018),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )

ellipse_map_21 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2021) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2021) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2021),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )
ellipse_map_18
ellipse_map_21
```


## Track Data


```{r load GPX data}
gpx_parsed_A <- htmlTreeParse(file = "data/Geo879_route_Annika.gpx", useInternalNodes = TRUE)
gpx_parsed_F <- htmlTreeParse(file = "data/Geo879_route_Fabienne.gpx", useInternalNodes = TRUE)
gpx_parsed_C <- htmlTreeParse(file = "data/Geo879_route_Cyril.gpx", useInternalNodes = TRUE)
```

Extract and store them in a more readable structure:
```{r extract info to dataframe}
# Track Annika
coords <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt/ele", fun = xmlValue)
ts_chr <- xpathSApply(gpx_parsed_A, path = "//trkpt/time", xmlValue)

track_A <- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = "UTC"),
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

# Add index to df
track_A$index <- 1:nrow(track_A)

# Track Fabi
coords <- xpathSApply(doc = gpx_parsed_F, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed_F, path = "//trkpt/ele", fun = xmlValue)
ts_chr <- xpathSApply(gpx_parsed_F, path = "//trkpt/time", xmlValue)

track_F <- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = "UTC"),
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

# Add index to df
track_F$index <- 1:nrow(track_F)
```

```{r convert to sf object and build trajectory}
track_sf_A <- st_as_sf(x = track_A, coords = c("lon","lat"), crs = 4326) %>% arrange(index)
mapview(track_sf_A)

track_sf_F <- st_as_sf(x = track_F, coords = c("lon","lat"), crs = 4326) %>% arrange(index)
mapview(track_sf_F)

# Creating trajectory by combining the coordinate points into multipoint feature
traj <- st_union(track_sf_A$geometry)
traj_sf_A <- st_sf(geometry = traj) %>% st_cast("LINESTRING")
# mapview(traj_sf) # NOT WORKING PROPERLY > ORDER IS NOT CORRECT
```



## Read Empatica 4 Data

# EDA - Electrodermal Activity (Unit: microsiemens)

```{r}
EDA_A <- read_csv("data/E4_data_Annika/EDA.csv")
EDA_F <- read_csv("data/E4_data_Fabienne/EDA.csv")
temp_A <- read_csv("data/E4_data_Annika/tags.csv")
```

# Following the tutorial to parse the E4 data https://rpubs.com/OliverSW/eda-analysis-tutorial
# Adding timestamp info

```{r write functions to parse E4 data and add timestamp}

read.empatica.eda <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(Timestamp=NA, EDA=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$EDA) 
  data$Timestamp <- timestamps
  data
}

read.empatica.acc <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(X=raw$V1[3:length(raw$V1)]/64.0,Y=raw$V2[3:length(raw$V2)]/64.0,Z=raw$V3[3:length(raw$V3)]/64.0)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$X) 
  data$Timestamp <- timestamps
  data
}

read.empatica.bvp <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(BVP=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$BVP) 
  data$Timestamp <- timestamps
  data
}

read.empatica.hr <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(seq_along(raw$V1[2:length(raw$V1)]),units = "secs")
  timestamps <- start+dt
  HR <- as.numeric(raw$V1[2:length(raw$V1)])
  data <- data.frame(Timestamp=timestamps, HR=HR)
  data
}

read.empatica.ibi <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(raw$V1[2:length(raw$V1)],units = "secs")
  timestamps <- start+dt
  ibi <- as.double(as.character(raw$V2[2:length(raw$V2)]) )
  data <- data.frame(Timestamp=timestamps, IBI=ibi)
  data
}

read.empatica.temp <- function(file) {
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  temperatureF <- (9.0/5.0)*(raw$V1[3:length(raw$V1)] + 32.0)
  data <- data.frame(TEMP=temperatureF)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$TEMP) 
  data$Timestamp <- timestamps
  data
}

read.empatica <- function(path) {
  FIELDS <- c("Z","Y","X","Battery","Temperature","EDA", "HR")
  FILES <- c("ACC.csv","BVP.csv","EDA.csv","TEMP.csv","IBI.csv", "HR.csv")
  #First lets read the file header info
  acc <- read.empatica.acc(file.path(path,"ACC.csv"))
  bvp <- read.empatica.bvp(file.path(path,"BVP.csv"))
  ibi <- read.empatica.ibi(file.path(path,"IBI.csv"))
  temp <- read.empatica.temp(file.path(path,"TEMP.csv"))
  eda <- read.empatica.eda(file.path(path,"EDA.csv"))
  hr <- read.empatica.hr((file.path(path, "HR.csv")))
  
  data <- list(ACC=acc,BVP=bvp,EDA=eda,TEMP=temp,IBI=ibi, HR=hr)
  attr(data,"class") <- "eda"
  return(data)
}
```

# Read EDA data

```{r read E4 data}
# Data Annika 
e4Data_A <- read.empatica("data/E4_data_Annika/")
EDA_A <- e4Data_A$EDA
EDA_A

# Heart rate data
HR_A <- e4Data_A$HR
HR_A

# Data Fabi
e4Data_F <- read.empatica("data/E4_data_Fabienne/")
EDA_F <- e4Data_F$EDA
EDA_F

HR_F <- e4Data_F$HR
HR_F
```

```{r visualizing EDA values}
plt_A <- qplot(data=EDA_A,x = Timestamp, y=EDA,geom="line",col="Annika") +
  xlab("Time") +
  ylab("GSR")
ggplotly(plt_A,width = 900,height=450)

plt_F <- qplot(data=EDA_F,x = Timestamp, y=EDA,geom="line",col="Fabienne") +
  xlab("Time") +
  ylab("GSR")
ggplotly(plt_F,width = 900,height=450)
```


```{r Filter & Smoothing}
threshold <- 1

# Annika
EDA_A$EDA[EDA_A$EDA  > threshold] <- 1

# EDA_A_filtered <- EDA_A |>
#   mutate(Q1 = quantile(EDA, 0.1),
#          Q3 = quantile(EDA, 0.9),
#          IQR = Q3 - Q1) |>
#   filter(EDA < (Q3 + 1.5 * IQR)) # Adjust the multiplier for more stringent or lenient filtering


window_size <- 5

EDA_A_smoothed <- EDA_A |>
  mutate(GSR_smoothed = rollmean(EDA, k = window_size, fill = NA))

plt_A <- qplot(data=EDA_A_smoothed,x = Timestamp, y=GSR_smoothed,geom="line",col="Annika") 
ggplotly(plt_A,width = 900,height=450)

# Fabienne

EDA_F$EDA[EDA_F$EDA  > threshold] <- 1

# Perform filtering on the transformed data
# EDA_F_filtered <- EDA_F |>
#   mutate(Q1 = quantile(log_eda, 0.25),
#          Q3 = quantile(log_eda, 0.75),
#          IQR = Q3 - Q1) |>
#   filter(EDA < (Q3 + 1.5 * IQR))

window_size <- 5

EDA_F_smoothed <- EDA_F |>
  mutate(GSR_smoothed = rollmean(EDA, k = window_size, fill = NA))

plt_F <- qplot(data=EDA_F_smoothed,x = Timestamp, y=GSR_smoothed,geom="line",col="Fabienne") 
ggplotly(plt_F, width = 900,height=450)
```

To precisely analyze the GSR signal and accurately identify GSR peaks within the time domain, it's essential to eliminate the tonic component from the raw signal.

```{r Tonic skin conductance level (SCL)}
# Assuming 'EDA_A_filtered' is a data frame with columns 'Timestamp' and 'EDA'

window_size <- 32  # Adjust the window size as needed

# https://tesi.univpm.it/retrieve/7fd49ab6-6325-406a-867d-68c75c1d2375/Thesis%20-%20Anna%20Brocanelli.pdf -> Window size according to page 30.

# Calculate the rolling mean using a window
EDA_A_smoothed$tonic_values <- rollmean(EDA_A_smoothed$EDA, k = window_size, align = "center", fill = NA)
EDA_F_smoothed$tonic_values <- rollmean(EDA_F_smoothed$EDA, k = window_size, align = "center", fill = NA)

# Define the file name for the plot
png(file = "plots/GSR_plot_Annika.png", width = 800, height = 600)

# Plotting the original EDA measurements and the smoothed trend
plot(EDA_A_smoothed$Timestamp, EDA_A_smoothed$EDA, type = 'l', col = 'blue', 
     xlab = 'Time',
     ylab = 'Galvanic Skin Response [µS]',
     main ="GSR and extracted Tonic GSR curve - Annika")

# Add the smoothed trend using lines()
lines(EDA_A_smoothed$Timestamp, EDA_A_smoothed$tonic_values, col = 'red', lwd = 2)

# Add legend
legend("topleft", legend = c("GSR", "Tonic GSR"), col = c("blue", "red"), lty = 1:2, lwd = c(1, 2))

dev.off()

# Define the file name for the plot
png(file = "plots/GSR_plot_Fabienne.png", width = 800, height = 600)

# Plot Fabienne with increased text size
plot(EDA_F_smoothed$Timestamp, EDA_F_smoothed$EDA, type = 'l', col = 'blue', 
     xlab = 'Time',
     ylab = 'Galvanic Skin Response [µS]',
     main = "GSR and extracted Tonic GSR curve - Fabienne",
     cex.axis = 1.2, cex.lab = 1.4, cex.main = 2)  # Adjust text size for axis labels and title

# Add the smoothed trend using lines()
lines(EDA_F_smoothed$Timestamp, EDA_F_smoothed$tonic_values, col = 'red', lwd = 2)

# Add legend with increased text size
legend("topleft", legend = c("GSR", "Tonic GSR"), col = c("blue", "red"), lty = 1:2, lwd = c(1, 2), cex = 1.2) 

# The 'cex' parameter is used to adjust the size of text elements



dev.off()
```

```{r Phasic skin conductance response (SCR)}

# Compute Phasic Values by subtracting tonic values from original values
EDA_A_smoothed$phasic_values <- EDA_A_smoothed$GSR_smoothed - EDA_A_smoothed$tonic_values
EDA_F_smoothed$phasic_values <- (EDA_F_smoothed$GSR_smoothed - EDA_F_smoothed$tonic_values)

plt_A_phasic <- qplot(data=EDA_A_smoothed,x = Timestamp, y=phasic_values,geom="line") +
  xlab("Time") +
  ylab("Phasic Values [µS]") +
  ggtitle("Extracted Phasic Values from GSR Measurement - Annika") +
  geom_hline(yintercept = 0.01, linetype = "dotted", color = "red", size = 0.5)

ggplotly(plt_A_phasic,width = 900,height=450)

plt_F_phasic <- qplot(data=EDA_F_smoothed,x = Timestamp, y=phasic_values,geom="line") +
  xlab("Time") +
  ylab("Phasic Values [µS]") +
  ggtitle("Extracted Phasic Values from GSR Measurement - Fabienne")+
  geom_hline(yintercept = 0.01, linetype = "dotted", color = "red", size = 0.5) 
  
ggplotly(plt_F_phasic,width = 900,height=450)
```

```{r}
# Extract peaks and timestamp
peaksize <- 0.01

# Find peaks exceeding the threshold
peak_positions_A <- which(EDA_A_smoothed$phasic_values > peaksize)
peak_timestamps_A <- EDA_A_smoothed$Timestamp[peak_positions_A]

peak_positions_F <- which(EDA_F_smoothed$phasic_values > peaksize)
peak_timestamps_F <- EDA_F_smoothed$Timestamp[peak_positions_F]


# Display the positions and timestamps of the peaks
peak_data_A <- data.frame(Position = peak_positions_A, Timestamp = peak_timestamps_A)
print(peak_data_A)

peak_data_F <- data.frame(Position = peak_positions_F, Timestamp = peak_timestamps_F)
print(peak_data_F)
```


```{r Track interpolation Annika}
track_sf_A1 <- track_sf_A
track_A1 <- track_A

# Make sure timestamp has correct format --> convert timestamp from Empatica data to UTC
track_sf_A1$ts_POSIXct <- as.POSIXct(track_sf_A$ts_POSIXct, tz = "UTC")
track_sf_A1$ts_POSIXct <- with_tz(track_sf_A$ts_POSIXct, tz = "Europe/Zurich")

track_A1$ts_POSIXct <- as.POSIXct(track_A$ts_POSIXct, tz = "UTC")
track_A1$ts_POSIXct <- with_tz(track_A$ts_POSIXct, tz = "Europe/Zurich")

# Interpolate track data to have a sampling rate of 1 Hz. 
expanded_track_A <- track_A1 %>%
  complete(ts_POSIXct = seq(min(ts_POSIXct), max(ts_POSIXct), by = "1 sec")) |>
  # Fills missing values generated by the
  fill(elevation, .direction = "down") |>  #interpolation
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

# Drop index column 
expanded_track_A <- expanded_track_A[,-5]

# Round timestamps to the nearest second to handle minor differences
peak_data_A$Timestamp <- round(peak_data_A$Timestamp, units = "secs")
expanded_track_A$ts_POSIXct <- round(expanded_track_A$ts_POSIXct, units = "secs")

# Join Track and Peaks
peaks_merged_A <- left_join(peak_data_A, expanded_track_A, by = c("Timestamp" = "ts_POSIXct"))

# Join Track and full EDA df
full_merged_A <- left_join(EDA_A_smoothed, expanded_track_A, by = c("Timestamp" = "ts_POSIXct")) |>
  fill(elevation, .direction = "down") |>
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

 full_merged_A <- full_merged_A[complete.cases(full_merged_A), ]

```

```{r Track interploation Fabi}
track_sf_F1 <- track_sf_F
track_F1 <- track_F

# Make sure timestamp has correct format --> convert timestamp from Empatica data to UTC
track_sf_F$ts_POSIXct <- as.POSIXct(track_sf_F$ts_POSIXct, tz = "UTC")
track_sf_F$ts_POSIXct <- with_tz(track_sf_F$ts_POSIXct, tz = "Europe/Zurich")

track_F$ts_POSIXct <- as.POSIXct(track_F$ts_POSIXct, tz = "UTC")
track_F$ts_POSIXct <- with_tz(track_F$ts_POSIXct, tz = "Europe/Zurich")

# Interpolate track data to have a sampling rate of 1 Hz. 
expanded_track_F <- track_F %>%
  complete(ts_POSIXct = seq(min(ts_POSIXct), max(ts_POSIXct), by = "1 sec")) |>
  # Fills missing values generated by the
  fill(elevation, .direction = "down") |>  #interpolation
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

# Drop index column 
expanded_track_F <- expanded_track_F[, -5]

# Round timestamps to the nearest second to handle minor differences
peak_data_F$Timestamp <- round(peak_data_F$Timestamp, units = "secs")
track_F$ts_POSIXct <- round(track_F$ts_POSIXct, units = "secs")

# Join Track and Peaks
peaks_merged_F <- left_join(peak_data_F, expanded_track_F, by = c("Timestamp" = "ts_POSIXct"))

# Join Track and full EDA df
full_merged_F <- left_join(EDA_F_smoothed, expanded_track_F, by = c("Timestamp" = "ts_POSIXct")) |>
  fill(elevation, .direction = "down") |>
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

 full_merged_F <- full_merged_F[complete.cases(full_merged_F), ]

```


```{r Merged SCR and track}
# Create SF Object
#### RUN AGAIN AFTER ERROR ####
full_merged_sf_A <- st_as_sf(x = full_merged_A, coords = c("lon","lat"), crs = 4326)
full_merged_sf_F <- st_as_sf(x = full_merged_F, coords = c("lon","lat"), crs = 4326)

# Create a plot with two spatial layers and color mapping for 'phasic_values'
ggplot() +
  geom_sf(data = full_merged_sf_A, aes(color = phasic_values)) +
  scale_color_gradient(low = "blue", high = "red")  

ggplot() +
  geom_sf(data = full_merged_sf_F, aes(color = phasic_values)) +
  scale_color_gradient(low = "blue", high = "red")  
```

## Visualization 

```{r Mapview plot - SRC on track}

color_palette <- colorRampPalette(c("blue", "white", "red"))(n = 100)

# create subset of dataframe filtering out strong outliers 
full_merged_sf_A_sub <- subset(full_merged_sf_A, phasic_values<0.06) %>% subset(., phasic_values>-0.06)

full_merged_sf_F_sub <- subset(full_merged_sf_F, phasic_values<0.06) %>% subset(., phasic_values>-0.06)

mapview(full_merged_sf_A_sub, 
        zcol = "phasic_values", 
        col.regions = color_palette, 
        cex = ifelse(full_merged_sf_A$phasic_values > 0.03, 5, 1), 
        lwd = 0.01, 
        layer.name = "Phasic SCR")

mapview(full_merged_sf_F_sub, 
        zcol = "phasic_values", 
        col.regions = color_palette, 
        cex = ifelse(full_merged_sf_F$phasic_values > 0.03, 5, 1), 
        lwd = 0.01, 
        layer.name = "Phasic SCR")
```
```{r REad bike network}
bike_network <- read_rds("data/full_zh_biking_network.rds")

## Extract Edges and add to plot
```

```{r, warning = F, fig.widht = 10, fig.height = 9}
library(sfnetworks)
edges <- bike_network %>% activate(edges) %>% st_as_sf()
fluss <- st_read("data/fluss/VEC200_FlowingWater.shp") |> filter(NAMN == "Limmat")

bbox_track <- st_bbox(track_sf_A) |> st_as_sfc() |> st_transform(crs = 2056)

zh_districts_intersect <- st_intersection(zh_districts, st_buffer(bbox_track,500))
edges_intersect <- st_intersection(edges, st_buffer(bbox_track,500)) |>
  filter(highway != "footway") |>
  filter(highway != "path")

tmap_mode("plot")
# Assuming you have loaded necessary libraries like 'sf' and 'tmap'

# Convert first_point and last_point to sf objects
first_point <- st_as_sf(track_sf_A[1, ]) %>%
  mutate(text = "Start")
last_point <- st_as_sf(track_sf_A[nrow(track_sf_A), ]) %>%
  mutate(text = "End")

# Create a tmap plot with points, lines, and text labels
track_map <- tm_shape(zh_districts_intersect) +
  tm_polygons() +
tm_shape(fluss) +
  tm_lines(col = "lightblue1", lwd = 10) +
tm_shape(edges_intersect) +
  tm_lines() +
tm_shape(track_sf_A) +
  tm_dots(col = "red", size = 0.02) +
tm_shape(first_point) +
  tm_dots(col = "red1", size = 0.5) +
  tm_text("text", size = 2,ymod = 0.6,xmod = 2, col = "red") +  # Specify the column containing text
tm_shape(last_point) +
  tm_dots(col = "red1", size = 0.5) +
  tm_text("text", size = 2, ymod = -0.8, col = "red") +  # Specify the column containing text
tm_layout(title = "Cycling Track", title.position = c("center", "top"), 
          inner.margins = c(0.08, 0.08, 0.08, 0.08),
          title.size = 2, 
          frame = F)

track_map
tmap_save(track_map, filename = "plots/cycling_track_map.png", width = 10, height = 8, units = "in")

```

## Statistical analysis of the SCR results 

```{r Boudning box and accident plotting}
peaks_merged_A <- peaks_merged_A[complete.cases(peaks_merged_A), ]

# Expanded Track Data
peaks_merged_sf_A <- st_as_sf(x = peaks_merged_A, coords = c("lon","lat"), crs = 4326)
mapview(peaks_merged_sf_A)

extent <- st_bbox(peaks_merged_sf_A)

tmap_mode("view")
tm_shape(peaks_merged_sf_A) +
  tm_dots(size = 0.1)

peaks_merged_F <- peaks_merged_F[complete.cases(peaks_merged_F), ]

# Fabi
peaks_merged_sf_F <- st_as_sf(x = peaks_merged_F, coords = c("lon","lat"), crs = 4326)
mapview(peaks_merged_sf_F)

extent <- st_bbox(peaks_merged_sf_F)

tmap_mode("plot")
tm_shape(b) +
  tm_dots(size = 0.1, col = "red")+
tm_shape(peaks_merged_sf_F) +
  tm_dots(size = 0.1)

```
```{r Plot: Peaks along track}
tmap_mode("view")

track_peak <- tm_shape(zh_districts_intersect) +
  tm_polygons() +
tm_shape(fluss) +
  tm_lines(col = "lightblue1", lwd = 10) +
tm_shape(edges_intersect) +
  tm_lines() +
tm_shape(track_sf_F) +
  tm_dots(col = "red", size = 0.01) +
tm_shape(first_point) +
  tm_dots(col = "red1", size = 0.1) +
tm_shape(last_point) +
  tm_dots(col = "red1", size = 0.1) +
tm_shape(peaks_merged_sf_F) +
  tm_dots(col = "blue", size = 0.1) +
tm_layout(title = "Cycling Track and GSR Peaks", title.position = c("center", "top"), 
          inner.margins = c(0.1, 0.1, 0.1, 0.1),
          title.size = 1, 
          frame = F)

track_peak
tmap_save(track_peak, filename = "plots/cycling_peak_map.png", width = 10, height = 8, units = "in")
```

```{r Buffer track and intersection}
# create buffer around set of coordinate points 
track_sf_A_buffer <- st_buffer(track_sf_A, dist = 30)
mapview(track_sf_A_buffer)

# merge buffer 
track_sf_A_buffer <- st_union(track_sf_A_buffer)
mapview(track_sf_A_buffer) + mapview(b, cex = 1)
# check buffer size and accident locations in mapview 

# transform CRS
track_sf_A_buffer <- st_transform(track_sf_A_buffer, st_crs(b))

# intersection of bike accidents b with buffer of track
acc_buffer <- st_intersection(b, track_sf_A_buffer)

mapview(acc_buffer, zcol = "AccidentSeverityCategory_en")
```


```{r Spatial point pattern analysis, warning=FALSE}

# Extract coordinates from the peak SCR value dataframe
peaks_A_coor <- st_transform(peaks_merged_sf_A[4], crs = 2056)
bike_acc_coor <- acc_buffer[37]

# Extract bounding box coordinates for the window
window_bounds <- st_bbox(peaks_A_coor)

# Create a rectangle window using the bounding box coordinates
window_bb <- owin(xrange = c(window_bounds["xmin"], window_bounds["xmax"]),
               yrange = c(window_bounds["ymin"], window_bounds["ymax"]))

# Convert spatial objects to point patterns
peaks_A_ppp <- as.ppp(st_coordinates(peaks_A_coor), W = window_bb)
accidents_ppp <- as.ppp(st_coordinates(bike_acc_coor), W = window_bb)

# Plot point patterns
plot(peaks_A_ppp, main = "Accidents and SCR Peaks along Track")
plot(accidents_ppp, main = "Accident Locations", add = TRUE, col = "red")
plot(peaks_A_ppp, main = "Skin Conductance Response Peaks", add = TRUE, col = "blue")
```

```{r Spatial Correlation}
# Combine both point patterns into a single multitype point pattern
combined_ppp <- superimpose(peaks_A_ppp, accidents_ppp)
is.multitype(combined_ppp)

marks(combined_ppp) <- factor(c(rep(1, npoints(peaks_A_ppp)), rep(2, npoints(accidents_ppp))))
# factor() in the context of spatial analysis, it's often used to assign different types or categories to points or observations within a dataset. Create multitype point patterns > assign marks of 1 to peaks_A_ppp points and marks of 2 to accidents_ppp points within the combined_ppp object.
is.multitype(combined_ppp)

# Compute the cross-K function for the multitype point pattern: 
# Multiple K function counts the expected number of points x within a given distance of a point of type y
cross_K <- Kcross(combined_ppp)
summary(cross_K)

# Try Ripley's K function (edge effect correction)
kf <- Kest(combined_ppp, correction = "Ripley")

png(file = "plots/Kcross.png", width = 800, height = 600)

plot(kf)

dev.off

# Plot the cross-K function
plot(cross_K, main = "Cross-K Function")
```
Interpretation: K cross function assesses the spatial correlation between different types of points. For example, it examines if one type of point tends to be closer or farther away from another type of point than expected under complete spatial randomness (CSR).

Summary output:
- r: Distance represents the distance range considered for analyzing spatial relationships.
- theo: theoretical values are expected values of K based on complete spatial randomness (CRS). In CSR, the expected number of pairs of points at different distances is calculated based on the overall point density and the window shape
- border: border correction values used to correct for edge effects or border conditions in spatial analysis. They account for the fact that points near the edge of the study area might have fewer neighbors within a certain distance.
- trans: transformation values in the context of spatial statistics might represent modifications applied to the original K function to better reflect spatial processes.
- iso: isotropic values refer to uniformity in all directions 

Interpretation of the plot: The plot shows that at all tested distances the actual observed value of K (iso) is greater than the theoretically derived expected value K(pois) indicating clustering.

```{r test spatial correlation}
# Extract theoretical and observed values
theoretical_values <- cross_K$theo
observed_values <- cross_K$r

# Calculate Pearson correlation
correlation_coefficient <- cor(theoretical_values, observed_values)
print(correlation_coefficient)
```
Pearson's correlation between observed and theoretical (CSR) values outputs a value of 0.968, which is near to 1 and hence indicates a very strong positive correlation. 

Discussion of spatial correlation: 

The fact that both datasets are recorded along a single bike track within a predefined buffer indeed implies a certain level of spatial correlation or association between these points. This arrangement suggests that they share a spatial context and are likely related due to their proximity within this specific area.

However, employing spatial analysis techniques such as the cross-K function doesn't just confirm the presence of correlation but also quantifies and characterizes the nature and degree of that correlation. It's crucial because:

1. Degree of Correlation: Even within a defined buffer or track, there might be variations in the strength or intensity of spatial association. Some regions might show stronger clustering, while others might exhibit more dispersion.

2. Assessing Significance: These analyses help in determining whether the observed spatial relationship is statistically significant or merely due to chance. Statistical significance provides confidence that the spatial association is not random.

3. Differentiation between Patterns: Spatial statistics can differentiate between distinct patterns within the buffer. For instance, even if both datasets are related to the same track, they might show different clustering tendencies or exhibit dissimilar spatial behaviors along that track.


## Comparison to HR data

```{r visualizing EDA values}
plt_HR_A <- qplot(data=HR_A,x = Timestamp, y=HR,geom="line",col="Annika") +
  xlab("Time") +
  ylab("Heart Rate")
ggplotly(plt_HR_A,width = 900,height=450)

plt_HR_F <- qplot(data=HR_F,x = Timestamp, y=HR,geom="line",col="Fabienne") +
  xlab("Time") +
  ylab("Heart Rate")
ggplotly(plt_HR_F,width = 900,height=450)


# Join HR data with full merged data
merged_A_HR_SCR <- left_join(HR_A, full_merged_A, by = "Timestamp")

# regression SCR and HR
reg_HR_SCR <- lm(phasic_values ~ HR, data = merged_A_HR_SCR)
summary(reg_HR_SCR)

# Plotting the regression line
plot(merged_A_HR_SCR$HR, merged_A_HR_SCR$phasic_values,
     xlab = "Heart Rate", ylab = "Phasic SCR Values",
     main = "Linear Regression between Heart Rate and Phasic SCR")

# Adding the regression line to the plot
abline(reg_HR_SCR, col = "red")

```
The F-statistic tests if at least one coefficient is significantly different from zero. A high p-value (0.9011) indicates that the model as a whole is not statistically significant. Neither the intercept nor the coefficient for 'HR' appears to have a significant impact on predicting 'phasic_values'. The low R-squared values and non-significant coefficients suggest that 'HR' doesn't have a substantial linear relationship with 'phasic_values' in this model.

--> Good!! we assumed that the HR would not significantly affect the phasic SCR variability

```{r Spatial proximity based on nearest neighbours}
# Since the points (acc and SCR peaks) don't share exact coordinates -> explore relationship based on spatial proximity using nearest neighbou approach:

full_merged_sf_A <- st_transform(full_merged_sf_A, st_crs(acc_buffer))

# Calculate nearest neighbor distances and indices
nearest_neighbors_A <- st_nearest_feature(acc_buffer, full_merged_sf_A)

# Merge SCR values to accident locations based on nearest neighbors
nn_A <- cbind(acc_buffer, nearest_neighbors_A)

# Extract corresponding SCR values based on nearest neighbor indices
nn_A$SCR_phasic <- full_merged_sf_A$phasic_values[nn_A$nearest_neighbors_A]
```

To predict the accident severity from phasic SCR values, we perform a linear regression model. Since we have categorical severity classes rather than continuous numerical values, we use dummy coding which leads to a table called contrast matrix. Generally, a categorical variable with n levels will be transformed into n-1 variables each with two levels. These n-1 new variables contain the same information than the single variable. This recoding creates a table called contrast matrix. > as.factor to factorize the categorical values.

Using one-way ANOVA (one predictor and response variable with multiple categories)

https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/ > they propose to use ANOVA: If you use only one continuous predictor, you could “flip” the model around so that, say, phasic SCR was the outcome variable and accident severity category was the predictor variable. This isn’t a bad thing to do if you only have one predictor variable (from the logistic model), and it is continuous.

```{r SCR distribution}

# Examine the distribution of SCR_phasic values for each accident severity category 

ggplot(nn_A, aes(x = AccidentSeverityCategory_en, y = SCR_phasic)) +
  geom_boxplot(width = 0.5, fill = "skyblue", color = "darkblue", alpha = 0.8) +
  geom_jitter(width = 0.2, alpha = 0.2, color = "red") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        axis.line = element_line(colour = "black"),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12, face = "bold"),
        legend.title = element_blank(),
        legend.position = "bottom") +
  labs(x = "Accident Severity Category", y = "SCR Phasic Values",
       title = "Distribution of SCR Phasic Values by Accident Severity") +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) 

```
```{r}
# Extracting X and Y coordinates from nn_A geom
coords <- st_coordinates(nn_A)

# Creating separate columns for X and Y
nn_A$X <- coords[, "X"]
nn_A$Y <- coords[, "Y"]
```


```{r ANOVA}
# One-way ANOVA
anova_result <- aov(SCR_phasic ~ as.factor(AccidentSeverityCategory_en), data = nn_A)

# Summary of ANOVA
summary(anova_result)
```

Summary interpretation: 

F value: With an F value of 0.672 and a corresponding p-value of 0.57, it indicates that the differences in mean SCR_phasic values between the categories of AccidentSeverityCategory_en are not statistically significant.

Conclusion: There's insufficient evidence to reject the null hypothesis. It suggests that, based on SCR_phasic values, the different categories of accident severity do not have significantly different mean values.


## To Do: 

- plot of heart rate
- volume of intersection of dbscan plot: intersection
- regression analysis between SCR phasic peaks and accident severity
