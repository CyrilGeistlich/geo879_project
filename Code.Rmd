---
title: "GEO879 - Mobility Issues Project"
author: "Cyril Geistlich, Annika Kunz, Fabienne Koenig"
date: "2023-10-11"
output: html_document
---

# Load Libraries

```{r setup, include=FALSE, warning = F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Install/load packages
## Default repository
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos = r)
})

check_pkg <- function(x)
  {
    if (!require(x, character.only = TRUE))
    {
      install.packages(x, dep = TRUE)
        if(!require(x, character.only = TRUE)) stop("Package not found")
    }
}

check_pkg("sf")
check_pkg("httr")
check_pkg("ggplot2")
check_pkg("plotly")
check_pkg("tidyverse")
check_pkg("leaflet")
check_pkg("dplyr")
check_pkg("XML")
check_pkg("mapview")
check_pkg("lubridate")
check_pkg("dplyr")
check_pkg("readr") 
check_pkg("tmap")
check_pkg("spatstat") 
check_pkg("dbscan")
check_pkg("ellipse")
check_pkg("phonTools")
check_pkg("devtools")
check_pkg("stringr")
check_pkg("zoo")
check_pkg("RColorBrewer")
check_pkg("spatstat")

```

```{r Projections}
# Projection strings for the Swiss LV03 & LV95 CRS and WGS84 CRS, respectively
crs_lv03  <- 21781
crs_lv95  <- 2056
crs_wgs84 <- 4326
```


## Read Data

```{r}
# I use .gpkg, since it is delivererd in Lv95 and no further tranformation is needed. 
accidents <- st_read( "data/RoadTrafficAccidentLocations.gpkg")
zh_boundary <- st_read( "data/Zurich_city_boundary.gpkg")
zh_districts <- st_read( "data/Zurich_city_districts.gpkg")
```
All layers are in CH1903+ / LV95. Therefore no further transformations are needed. 

## Data Preview

```{r Tables}
# a)
accidents |> ggplot() +
  geom_bar(aes(AccidentSeverityCategory_en)) +
  xlab("Accident Severity Category") +
  ggtitle("Number of Accidents by Severity") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# b)
accidents |> ggplot() +
  geom_bar(aes(AccidentType_en)) +
  xlab("Accident Category") +
  ggtitle("Number of Accidents by Category") +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1))

# c)

p <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == F)

b <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

m <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

pb <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == F)

pm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == F &
         AccidentInvolvingMotorcycle == T)

bm <- accidents |>
  filter(AccidentInvolvingPedestrian == F &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)

pbm <- accidents |>
  filter(AccidentInvolvingPedestrian == T &
         AccidentInvolvingBicycle == T &
         AccidentInvolvingMotorcycle == T)


counts_table <- data.frame(
  Type = c("Pedestrian","Bicycle","Motorcycle","Pedestrian & Bicycle", "Pedestrian & Motorcycle", "Bicycle & Motorcycle", "Pedestrian & Bicycle & Motorcycle"),
  Count = c(nrow(p),nrow(b),nrow(m),nrow(pb), nrow(pm), nrow(bm), nrow(pbm))
)

print(counts_table)


```

## Temporal Evolution

```{r temporal evolution plot}

temporal_plot <- accidents |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

temporal_plot_b <- b |>
  group_by(AccidentYear) |>
  summarise(Count = n()) |>
  ggplot() +
  geom_line(aes(x = AccidentYear, y = Count))

combined_plot <- ggplot() +
  geom_line(data = temporal_plot$data, aes(x = AccidentYear, y = Count, linetype = "All Accidents"), size = 1, color = "black") +
  geom_line(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, linetype = "Bicycle Accidents"), size = 1, color = "red") +
  geom_text(data = temporal_plot$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, hjust = -0.5, size = 3, color = "black") +
  geom_text(data = temporal_plot_b$data, aes(x = AccidentYear, y = Count, label = Count), vjust = -1, size = 3, color = "red") +
  xlab("Year") +
  ylab("Number of Accidents") +
  ggtitle("Accidents Over the Years") +
  scale_x_continuous(breaks = unique(accidents$AccidentYear)) +
  scale_linetype_manual(values = c("All Accidents" = "solid", "Bicycle Accidents" = "solid")) +
  guides(linetype = guide_legend(override.aes = list(color = c("black", "red"))) )  +
  scale_color_manual(values = c("All Accidents" = "black", "Bicycle Accidents" = "red")) +
  labs(linetype = "Transport Mode Type") + 
  geom_point(data = temporal_plot$data,  aes(x = AccidentYear, y = Count)) +
  geom_point(data = temporal_plot_b$data,  aes(x = AccidentYear, y = Count), color = "red")

print(combined_plot)

```

## Accident Severity

```{r Tmap Accident Severity}

tmap_mode("view")

basemap_imagery <-tm_basemap("Esri.WorldTopoMap") +
  tm_basemap("Esri.WorldImagery") +
  tm_basemap("Esri.WorldStreetMap") +
  tm_basemap("Esri.WorldGrayCanvas")

accident_map <- tm_shape(b) +
  tm_dots(col = "AccidentSeverityCategory_en", palette = "Set3", size = 0.02) + 
  tm_layout(
    title = "Accident Severity Map of Zurich", 
    title.position = c('left', 'top'), 
    legend.frame = T
  ) +
  tm_shape(zh_boundary) +
  tm_borders() +
  tm_shape(zh_districts) +
  tm_borders() +
  basemap_imagery 

accident_map

```

# Clustering of Accidents

Most accidents occur on or close to intersections. We have accumulations of accidents along larger streets. This might be the case because those streets might be cycled on more frequently and at a higher average velocity. 

```{r dbscan}

sf_data <- b
years <- c(2018, 2019, 2020, 2021)
cluster_maps <- list()

for (year in years) {
  data_year <- sf_data[sf_data$AccidentYear == year, ]
  coordinates_df <- st_coordinates(data_year)

  dbscan_result <- dbscan(coordinates_df, eps = 125)

  # move cluster 0 to own layer
  data_year_cluster0 <- data_year[dbscan_result$cluster == 0, ]
  data_year_cluster0$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster == 0])
  
  data_year <- data_year[dbscan_result$cluster != 0, ]
  data_year$Cluster <- as.factor(dbscan_result$cluster[dbscan_result$cluster != 0])

  cluster_map <- tm_shape(data_year) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tm_shape(data_year_cluster0) +
    tm_dots(col = "Cluster", size = 0.01) +
    tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", year),
      title.position = c('left', 'top'),
      legend.frame = TRUE
    )

  cluster_maps[[as.character(year)]] <- cluster_map

  assign(paste0("b_", year), data_year)
}

for (year in years) {
  print(cluster_maps[[as.character(year)]])
}

```


Cluster 0 was filtered out, since the data points in the cluster were spread out through the whole city. After removing the other smaller clusters become better visible in the maps. The clusters are along larger roads or at large intersection with a lot of traffic.

```{r SDE, include = F}

# Create a list to store ellipses
ellipses <- data.frame()
clusters <- unique(b_2018$Cluster)
ellipses_list <- list()

# 2018
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2018[b_2018$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
    
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2018 <- do.call(rbind, ellipses_list)
}

# 2021
clusters <- unique(b_2021$Cluster)
  # Create and add ellipses for each cluster
  for (cluster in clusters) {
    sde_cluster <- b_2021[b_2021$Cluster == cluster, ]
    coords <- as.matrix(st_coordinates(st_as_sf(sde_cluster)))
  
    # Create standard deviation ellipse
    plot.new()
    ellipse_data <- as.data.frame(sdellipse(coords))
  
    # Create the polygon object
    polygon <- ellipse_data |>
    st_as_sf(coords = c("V1", "V2"), crs = 2056) |>
    summarise((geometry = st_combine(geometry))) |>
    st_cast("POLYGON")
  
    polygon$Cluster <- cluster
  
    # Add the polygon to the list
    ellipses_list[[cluster]] <- polygon
    ellipses_2021 <- do.call(rbind, ellipses_list)
}

```


```{r ellipse map}
ellipse_map_18 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2018) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2018) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2018),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )

ellipse_map_21 <- tm_shape(zh_boundary) +
    tm_borders() +
    tm_shape(zh_districts) +
    tm_borders() +
    tm_shape(ellipses_2021) +
    tm_polygons(alpha = 0.5, palette = "Set1") +
    tm_shape(b_2021) +
    tm_dots(col = "Cluster", palette = "Set1", size = 0.01) +
    tmap_options(check.and.fix = TRUE) +
    tm_layout(
      title = paste("DBSCAN Clustering Map - Year", 2021),
      title.position = c('left', 'top'),
      legend.frame = TRUE
  )
ellipse_map_18
ellipse_map_21
```


## Track Data


```{r load GPX data}
gpx_parsed_A <- htmlTreeParse(file = "data/Geo879_route_Annika.gpx", useInternalNodes = TRUE)
gpx_parsed_F <- htmlTreeParse(file = "data/Geo879_route_Fabienne.gpx", useInternalNodes = TRUE)
gpx_parsed_C <- htmlTreeParse(file = "data/Geo879_route_Cyril.gpx", useInternalNodes = TRUE)
```

Extract and store them in a more readable structure:
```{r extract info to dataframe}
# Track Annika
coords <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed_A, path = "//trkpt/ele", fun = xmlValue)
ts_chr <- xpathSApply(gpx_parsed_A, path = "//trkpt/time", xmlValue)

track_A <- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = "UTC"),
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

# Add index to df
track_A$index <- 1:nrow(track_A)

# Track Fabi
coords <- xpathSApply(doc = gpx_parsed_F, path = "//trkpt", fun = xmlAttrs)
elevation <- xpathSApply(doc = gpx_parsed_F, path = "//trkpt/ele", fun = xmlValue)
ts_chr <- xpathSApply(gpx_parsed_F, path = "//trkpt/time", xmlValue)

track_F <- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = "UTC"),
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  elevation = as.numeric(elevation)
)

# Add index to df
track_F$index <- 1:nrow(track_F)
```

```{r convert to sf object and build trajectory}
track_sf_A <- st_as_sf(x = track_A, coords = c("lon","lat"), crs = 4326) %>% arrange(index)
mapview(track_sf_A)

track_sf_F <- st_as_sf(x = track_F, coords = c("lon","lat"), crs = 4326) %>% arrange(index)
mapview(track_sf_F)

# Creating trajectory by combining the coordinate points into multipoint feature
traj <- st_union(track_sf_A$geometry)
traj_sf_A <- st_sf(geometry = traj) %>% st_cast("LINESTRING")
# mapview(traj_sf) # NOT WORKING PROPERLY > ORDER IS NOT CORRECT
```

## Read Empatica 4 Data

# EDA - Electrodermal Activity (Unit: microsiemens)

```{r}
EDA_A <- read_csv("data/E4_data_Annika/EDA.csv")
EDA_F <- read_csv("data/E4_data_Fabienne/EDA.csv")
temp_A <- read_csv("data/E4_data_Annika/tags.csv")
```

# Following the tutorial to parse the E4 data https://rpubs.com/OliverSW/eda-analysis-tutorial
# Adding timestamp info

```{r write functions to parse E4 data and add timestamp}

read.empatica.eda <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(Timestamp=NA, EDA=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$EDA) 
  data$Timestamp <- timestamps
  data
}

read.empatica.acc <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(X=raw$V1[3:length(raw$V1)]/64.0,Y=raw$V2[3:length(raw$V2)]/64.0,Z=raw$V3[3:length(raw$V3)]/64.0)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$X) 
  data$Timestamp <- timestamps
  data
}

read.empatica.bvp <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  data <- data.frame(BVP=raw$V1[3:length(raw$V1)])
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$BVP) 
  data$Timestamp <- timestamps
  data
}

read.empatica.hr <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(seq_along(raw$V1[2:length(raw$V1)]),units = "secs")
  timestamps <- start+dt
  HR <- as.numeric(raw$V1[2:length(raw$V1)])
  data <- data.frame(Timestamp=timestamps, HR=HR)
  data
}

read.empatica.ibi <- function(file){
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  start <- as.POSIXct(x = start_s,origin = "1970-01-01")
  dt <- as.difftime(raw$V1[2:length(raw$V1)],units = "secs")
  timestamps <- start+dt
  ibi <- as.double(as.character(raw$V2[2:length(raw$V2)]) )
  data <- data.frame(Timestamp=timestamps, IBI=ibi)
  data
}

read.empatica.temp <- function(file) {
  raw <- read.csv(file,header = F)
  start_s <- raw$V1[1]
  sample_rate <- as.numeric(raw$V1[2])
  temperatureF <- (9.0/5.0)*(raw$V1[3:length(raw$V1)] + 32.0)
  data <- data.frame(TEMP=temperatureF)
  start <- as.POSIXct(x = start_s, origin = "1970-01-01")
  dt <- as.difftime(as.character(1.0/sample_rate),format = "%OS")
  timestamps <- seq(from = start, by=dt , along.with = data$TEMP) 
  data$Timestamp <- timestamps
  data
}

read.empatica <- function(path) {
  FIELDS <- c("Z","Y","X","Battery","Temperature","EDA", "HR")
  FILES <- c("ACC.csv","BVP.csv","EDA.csv","TEMP.csv","IBI.csv", "HR.csv")
  #First lets read the file header info
  acc <- read.empatica.acc(file.path(path,"ACC.csv"))
  bvp <- read.empatica.bvp(file.path(path,"BVP.csv"))
  ibi <- read.empatica.ibi(file.path(path,"IBI.csv"))
  temp <- read.empatica.temp(file.path(path,"TEMP.csv"))
  eda <- read.empatica.eda(file.path(path,"EDA.csv"))
  hr <- read.empatica.hr((file.path(path, "HR.csv")))
  
  data <- list(ACC=acc,BVP=bvp,EDA=eda,TEMP=temp,IBI=ibi, HR=hr)
  attr(data,"class") <- "eda"
  return(data)
}
```

# Read EDA data

```{r read E4 data}
# Data Annika 
e4Data_A <- read.empatica("data/E4_data_Annika/")
EDA_A <- e4Data_A$EDA
EDA_A

# Heart rate data
HR_A <- e4Data_A$HR
HR_A

# Data Fabi
e4Data_F <- read.empatica("data/E4_data_Fabienne/")
EDA_F <- e4Data_F$EDA
EDA_F

HR_F <- e4Data_F$HR
HR_F
```

```{r visualizing EDA values}
plt_A <- qplot(data=EDA_A,x = Timestamp, y=EDA,geom="line",col="Annika") +
  xlab("Time") +
  ylab("GSR")
ggplotly(plt_A,width = 900,height=450)

plt_F <- qplot(data=EDA_F,x = Timestamp, y=EDA,geom="line",col="Fabienne") +
  xlab("Time") +
  ylab("GSR")
ggplotly(plt_F,width = 900,height=450)
```


```{r Filter & Smoothing}
# Annika
EDA_A_filtered <- EDA_A |>
  mutate(Q1 = quantile(EDA, 0.25),
         Q3 = quantile(EDA, 0.75),
         IQR = Q3 - Q1) |>
  filter(EDA < (Q3 + 1.5 * IQR)) # Adjust the multiplier for more stringent or lenient filtering


window_size <- 5

EDA_A_smoothed <- EDA_A_filtered |>
  mutate(GSR_smoothed = rollmean(EDA, k = window_size, fill = NA))

plt_A <- qplot(data=EDA_A_smoothed,x = Timestamp, y=GSR_smoothed,geom="line",col="Annika") 
ggplotly(plt_A,width = 900,height=450)

# Fabienne
EDA_F_filtered <- EDA_F |>
  mutate(Q1 = quantile(EDA, 0.25),
         Q3 = quantile(EDA, 0.75),
         IQR = Q3 - Q1) |>
  filter(EDA < (Q3 + 1.5 * IQR)) # Adjust the multiplier for more stringent or lenient filtering


window_size <- 5

EDA_F_smoothed <- EDA_F_filtered |>
  mutate(GSR_smoothed = rollmean(EDA, k = window_size, fill = NA))

plt_F <- qplot(data=EDA_F_smoothed,x = Timestamp, y=GSR_smoothed,geom="line",col="Annika") 
ggplotly(plt_F, width = 900,height=450)
```

To precisely analyze the GSR signal and accurately identify GSR peaks within the time domain, it's essential to eliminate the tonic component from the raw signal.

```{r Tonic skin conductance level (SCL)}
# Assuming 'EDA_A_filtered' is a data frame with columns 'Timestamp' and 'EDA'

window_size <- 32  # Adjust the window size as needed

# https://tesi.univpm.it/retrieve/7fd49ab6-6325-406a-867d-68c75c1d2375/Thesis%20-%20Anna%20Brocanelli.pdf -> Window size according to page 30.

# Calculate the rolling mean using a window
EDA_A_smoothed$tonic_values <- rollmean(EDA_A_smoothed$EDA, k = window_size, align = "center", fill = NA)

# Plotting the original EDA measurements and the smoothed trend
plot(EDA_A_filtered$Timestamp, EDA_A_smoothed$EDA, type = 'l', col = 'blue', 
     xlab = 'Time',
     ylab = 'Galvanic Skin Response [µS]',
     main ="GSR and extracted Tonic GSR curve")

# Add the smoothed trend using lines()
lines(EDA_A_filtered$Timestamp, EDA_A_smoothed$tonic_values, col = 'red', lwd = 2)

# Add legend
legend("topleft", legend = c("GSR", "Tonic GSR"), col = c("blue", "red"), lty = 1:2, lwd = c(1, 2))
```

```{r Phasic skin conductance response (SCR)}
# Compute Phasic Values by subtracting tonic values from original values
EDA_A_smoothed$phasic_values <- EDA_A_smoothed$GSR_smoothed - EDA_A_smoothed$tonic_values

plt_A_phasic <- qplot(data=EDA_A_smoothed,x = Timestamp, y=phasic_values,geom="line",col="Annika") +
  xlab("Time") +
  ylab("Phasic Values [µS]") +
  ggtitle("Extracted Phasic Values from GSR Measurement")

ggplotly(plt_A_phasic,width = 900,height=450)
```

```{r}
# Extract peaks and timestamp
peaksize <- 0.01

# Find peaks exceeding the threshold
peak_positions_A <- which(EDA_A_smoothed$phasic_values > peaksize)
peak_timestamps_A <- EDA_A_smoothed$Timestamp[peak_positions_A]

# Display the positions and timestamps of the peaks
peak_data_A <- data.frame(Position = peak_positions_A, Timestamp = peak_timestamps_A)
print(peak_data_A)

```


```{r}

# Make sure timestamp has correct format --> convert timestamp from Empatica data to UTC
track_sf_A$ts_POSIXct <- as.POSIXct(track_sf_A$ts_POSIXct, tz = "UTC")
track_sf_A$ts_POSIXct <- with_tz(track_sf_A$ts_POSIXct, tz = "Europe/Zurich")

track_A$ts_POSIXct <- as.POSIXct(track_A$ts_POSIXct, tz = "UTC")
track_A$ts_POSIXct <- with_tz(track_A$ts_POSIXct, tz = "Europe/Zurich")


# Interpolate track data to have a sampling rate of 1 Hz. 
expanded_track_A <- track_A %>%
  complete(ts_POSIXct = seq(min(ts_POSIXct), max(ts_POSIXct), by = "1 sec")) |>
  # Fills missing values generated by the
  fill(elevation, .direction = "down") |>  #interpolation
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

# Drop index column 
expanded_track_A <- expanded_track_A[, -5]

# Round timestamps to the nearest second to handle minor differences
peak_data_A$Timestamp <- round(peak_data_A$Timestamp, units = "secs")
track_A$ts_POSIXct <- round(track_A$ts_POSIXct, units = "secs")

# Join Track and Peaks
peaks_merged_A <- left_join(peak_data_A, expanded_track_A, by = c("Timestamp" = "ts_POSIXct"))

# Join Track and full EDA df
full_merged_A <- left_join(EDA_A_smoothed, expanded_track_A, by = c("Timestamp" = "ts_POSIXct")) |>
  fill(elevation, .direction = "down") |>
  fill(lat, .direction = "downup") |>
  fill(lon, .direction = "downup")

```

```{r}

# Create SF Object
full_merged_sf_A <- st_as_sf(x = full_merged_A, coords = c("lon","lat"), crs = 4326)

# Create a plot with two spatial layers and color mapping for 'phasic_values'
ggplot() +
  geom_sf(data = full_merged_sf_A, aes(color = phasic_values)) +
  scale_color_gradient(low = "blue", high = "red")  # Adjust colors as desired


# Mapview plot:
color_palette <- colorRampPalette(c("blue", "white", "red"))(n = 100)

mapview(full_merged_sf_A, 
        zcol = "phasic_values", 
        col.regions = color_palette, 
        cex = ifelse(full_merged_sf_A$phasic_values > 0.01, 3, 1), 
        lwd = 0.01, 
        layer.name = "Phasic SCR")

mapview_SCR_A@map$col$legend$title <- "Custom Legend Title"

```

## Statistical analysis of the SCR results 

```{r}
bike_network <- read_rds("data/full_zh_biking_network.rds")

## Extract Edges and add to plot
```

```{r}
# Expanded Track Data
peaks_merged_sf_A <- st_as_sf(x = peaks_merged_A, coords = c("lon","lat"), crs = 4326)
mapview(peaks_merged_sf_A)

extent <- st_bbox(peaks_merged_sf_A)

tmap_mode("view")
tm_shape(peaks_merged_sf_A) +
  tm_dots(size = 0.1)
```


```{r Spatial point pattern analysis, warning=FALSE}

# Extract coordinates from the peak SCR value dataframe
peaks_A_coor <- st_transform(peaks_merged_sf_A[4], crs = 2056)
bike_acc_coor <- b[37]

# Extract bounding box coordinates for the window
window_bounds <- st_bbox(peaks_A_coor)

# Create a rectangle window using the bounding box coordinates
window_bb <- owin(xrange = c(window_bounds["xmin"], window_bounds["xmax"]),
               yrange = c(window_bounds["ymin"], window_bounds["ymax"]))

# Convert spatial ojects to point patterns
peaks_A_ppp <- as.ppp(st_coordinates(peaks_A_coor), W = window_bb)
accidents_ppp <- as.ppp(st_coordinates(bike_acc_coor), W = window_bb)

# Plot point patterns
plot(peaks_A_ppp, main = "Skin Conductance Response Peaks")
plot(accidents_ppp, main = "Accident Locations", add = TRUE, col = "red")
```

```{r Spatial Correlation}
# Combine both point patterns into a single multitype point pattern
combined_ppp <- superimpose(peaks_A_ppp, accidents_ppp)
is.multitype(combined_ppp)

marks(combined_ppp) <- factor(c(rep(1, npoints(peaks_A_ppp)), rep(2, npoints(accidents_ppp))))
# factor() in the context of spatial analysis, it's often used to assign different types or categories to points or observations within a dataset. Create multitype point patterns > assign marks of 1 to peaks_A_ppp points and marks of 2 to accidents_ppp points within the combined_ppp object.
is.multitype(combined_ppp)

# Compute the cross-K function for the multitype point pattern: 
# Multiple K function counts the expected number of points x within a given distance of a point of type y
cross_K <- Kcross(combined_ppp)
summary(cross_K)

# Try Ripley's K function (edge effect correction)
kf <- Kest(combined_ppp, correction = "Ripley")
plot(kf)

# Plot the cross-K function
plot(cross_K, main = "Cross-K Function")
```
Interpretation: K cross function assesses the spatial correlation between different types of points. For example, it examines if one type of point tends to be closer or farther away from another type of point than expected under complete spatial randomness (CSR).

Summary output:
- r: Distance represents the distance range considered for analyzing spatial relationships.
- theo: theoretical values are expected values of K based on complete spatial randomness (CRS). In CSR, the expected number of pairs of points at different distances is calculated based on the overall point density and the window shape
- border: border correction values used to correct for edge effects or border conditions in spatial analysis. They account for the fact that points near the edge of the study area might have fewer neighbors within a certain distance.
- trans: transformation values in the context of spatial statistics might represent modifications applied to the original K function to better reflect spatial processes.
- iso: isotropic values refer to uniformity in all directions 

Interpretation of the plot: The plot shows that at all tested distances the actual observed value of K (iso) is greater than the theoretically derived expected value K(pois) indicating clustering.

```{r test spatial correlation}
# Extract theoretical and observed values
theoretical_values <- cross_K$theo
observed_values <- cross_K$r

# Calculate Pearson correlation
correlation_coefficient <- cor(theoretical_values, observed_values)
print(correlation_coefficient)

```
Pearson's correlation between observed and theoretical (CSR) values outputs a value of 0.968, which is near to 1 and hence indicates a very strong positive correlation. 

## Comparison to HR data

```{r visualizing EDA values}
plt_HR_A <- qplot(data=HR_A,x = Timestamp, y=HR,geom="line",col="Annika") +
  xlab("Time") +
  ylab("Heart Rate")
ggplotly(plt_HR_A,width = 900,height=450)

plt_HR_F <- qplot(data=HR_F,x = Timestamp, y=HR,geom="line",col="Fabienne") +
  xlab("Time") +
  ylab("Heart Rate")
ggplotly(plt_HR_F,width = 900,height=450)
```


To Do: 

- plot of heart rate
- volume of intersection of dbscan plot: intersection
